from pydantic import BaseModel
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from typing import Dict, Any, List, Optional
import json
import asyncio
import os
import uuid
import threading

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv
load_dotenv()

# Google Generative AI SDK
import google.generativeai as genai

# æœ¬åœ°æ¨¡å‹åŒ¯å…¥
from models.SentimentModel import SentimentModel
from models.StressModel import StressModel

# === é…ç½® ===
TOTAL_QUESTIONS = 3  # ç¸½å•é¡Œæ•¸
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
MODEL_NAME = "gemini-2.0-flash"

# é…ç½® Gemini
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    print("è­¦å‘Šï¼šæœªè¨­å®š GOOGLE_API_KEY")

# FastAPI æ‡‰ç”¨
app = FastAPI(title="ç†è²¡å•å· API", version="1.0.0")

# CORS ä¸­ä»‹è»Ÿé«”
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # é–‹ç™¼ç’°å¢ƒï¼Œç”Ÿç”¢ç’°å¢ƒè«‹é™åˆ¶ origin
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨åŸŸè®Šæ•¸
SESSIONS: Dict[str, Dict[str, Any]] = {}
SESSIONS_LOCK = threading.Lock()
SENTIMENT_MODEL: Optional[SentimentModel] = None
STRESS_MODEL: Optional[StressModel] = None

# === API æ¨¡å‹ ===
class StartResponse(BaseModel):
    session_id: str
    # ç§»é™¤ questionï¼Œæ”¹ç”±ä¸²æµç«¯é»æä¾›

class AnswerRequest(BaseModel):
    session_id: str
    answer_text: str

class NextQuestionResponse(BaseModel):
    next_question: Optional[str] = None
    finished: bool = False
    advice: Optional[str] = None

class StreamQuestionRequest(BaseModel):
    session_id: str
    question_number: int

# === Gemini ç›¸é—œå‡½å¼ ===
async def stream_question_generation(prompt: str):
    """ä¸²æµæ–¹å¼ç”Ÿæˆå•é¡Œ"""
    if not GOOGLE_API_KEY:
        # Mock ä¸²æµå›æ‡‰ - æ¨¡æ“¬å•é¡Œç”Ÿæˆ
        import random
        mock_questions = [
            "è«‹ç°¡å–®æè¿°æ‚¨çš„æŠ•è³‡ç¶“é©—ï¼Ÿ",
            "é¢å°æŠ•è³‡è™§ææ™‚æ‚¨æœƒå¦‚ä½•åæ‡‰ï¼Ÿ", 
            "æ‚¨æŠ•è³‡çš„ä¸»è¦ç›®æ¨™æ˜¯ä»€éº¼ï¼Ÿ",
            "æ‚¨æ¯æœˆå¯æŠ•è³‡çš„é‡‘é¡å¤§ç´„å¤šå°‘ï¼Ÿ",
            "æ‚¨åå¥½å“ªç¨®æŠ•è³‡æ–¹å¼ï¼Ÿ"
        ]
        question = random.choice(mock_questions)
        for char in question:
            yield f"data: {json.dumps({'text': char, 'done': False}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.05)  # æ§åˆ¶é¡¯ç¤ºé€Ÿåº¦
        yield f"data: {json.dumps({'text': '', 'done': True}, ensure_ascii=False)}\n\n"
        return
    
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                temperature=0.7,
                max_output_tokens=1024,
            ),
            stream=True  # å•Ÿç”¨ä¸²æµ
        )
        
        for chunk in response:
            if chunk.text:
                # é€å­—ç™¼é€
                for char in chunk.text:
                    yield f"data: {json.dumps({'text': char, 'done': False}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.03)  # æ§åˆ¶é¡¯ç¤ºé€Ÿåº¦
        
        # ç™¼é€å®Œæˆä¿¡è™Ÿ
        yield f"data: {json.dumps({'text': '', 'done': True}, ensure_ascii=False)}\n\n"
        
    except Exception as e:
        print(f"Gemini ä¸²æµ API éŒ¯èª¤: {e}")
        error_msg = "(ç³»çµ±ç™¼ç”ŸéŒ¯èª¤ï¼Œç„¡æ³•å–å¾—å»ºè­°)"
        for char in error_msg:
            yield f"data: {json.dumps({'text': char, 'done': False}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.05)
        yield f"data: {json.dumps({'text': '', 'done': True}, ensure_ascii=False)}\n\n"

def call_gemini(prompt: str, is_question: bool = False) -> str:
    """å‘¼å« Gemini ç”Ÿæˆå…§å®¹"""
    if not GOOGLE_API_KEY:
        # Mock å›æ‡‰ - æä¾›åŸºæœ¬çš„å•é¡Œè¼ªæ›¿
        if is_question:
            import random
            mock_questions = [
                "è«‹ç°¡å–®æè¿°æ‚¨çš„æŠ•è³‡ç¶“é©—ï¼Ÿ",
                "é¢å°æŠ•è³‡è™§ææ™‚æ‚¨æœƒå¦‚ä½•åæ‡‰ï¼Ÿ", 
                "æ‚¨æŠ•è³‡çš„ä¸»è¦ç›®æ¨™æ˜¯ä»€éº¼ï¼Ÿ",
                "æ‚¨æ¯æœˆå¯æŠ•è³‡çš„é‡‘é¡å¤§ç´„å¤šå°‘ï¼Ÿ",
                "æ‚¨åå¥½å“ªç¨®æŠ•è³‡æ–¹å¼ï¼Ÿ"
            ]
            return random.choice(mock_questions)
        else:
            return "æ ¹æ“šæ‚¨çš„æƒ…ç·’èˆ‡å£“åŠ›åˆ†æï¼Œå»ºè­°ï¼šä¿æŒå¤šæ¨£åŒ–æŠ•è³‡çµ„åˆã€è¨­å®šé©ç•¶åœæé»ã€é™ä½æŠ•è³‡æ§“æ¡¿ï¼Œä¸¦å°‹æ±‚å°ˆæ¥­ç†è²¡è«®è©¢ã€‚"
    
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                temperature=0.7,
                max_output_tokens=1024,
            )
        )
        
        if response.text:
            return response.text.strip()
        else:
            return "(ç³»çµ±æš«æ™‚ç„¡æ³•ç”Ÿæˆå›æ‡‰ï¼Œè«‹ç¨å¾Œå†è©¦)"
            
    except Exception as e:
        print(f"Gemini API éŒ¯èª¤: {e}")
        if "quota" in str(e).lower():
            return "(API é…é¡å·²ç”¨å®Œï¼Œè«‹ç¨å¾Œå†è©¦)"
        elif "permission" in str(e).lower():
            return "(API é‡‘é‘°æ¬Šé™ä¸è¶³ï¼Œè«‹æª¢æŸ¥è¨­å®š)"
        else:
            return "(ç³»çµ±ç™¼ç”ŸéŒ¯èª¤ï¼Œç„¡æ³•å–å¾—å»ºè­°)"

def generate_question(question_number: int, previous_answers: List[str] = None) -> str:
    """å‹•æ…‹ç”ŸæˆæŠ•è³‡ç†è²¡ç›¸é—œå•é¡Œ"""
    # åŸºç¤å•é¡Œæç¤ºè©ï¼Œèšç„¦æ–¼æŠ•è³‡å¿ƒç†å’Œæƒ…ç·’
    base_prompts = [
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡æ™‚å¿ƒç†å£“åŠ›çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…æŠ•è³‡æ™‚çš„å£“åŠ›æ„Ÿå—ã€‚",
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡æƒ…ç·’æ³¢å‹•çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…é¢å°å¸‚å ´è®ŠåŒ–æ™‚çš„æƒ…ç·’åæ‡‰ã€‚",
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡ç„¦æ…®çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…åœ¨æŠ•è³‡éç¨‹ä¸­çš„æ“”æ†‚å’Œææ‡¼ã€‚",
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡æ±ºç­–å¿ƒç†çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…åšæŠ•è³‡æ±ºå®šæ™‚çš„å¿ƒç†ç‹€æ…‹ã€‚",
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡å£“åŠ›ç®¡ç†çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…å¦‚ä½•è™•ç†æŠ•è³‡å¸¶ä¾†çš„å¿ƒç†è² æ“”ã€‚"
    ]
    
    # å¦‚æœæœ‰å‰é¢çš„å›ç­”ï¼ŒåŠ å…¥ä¸Šä¸‹æ–‡ä¾†ç”Ÿæˆæ›´é‡å°æ€§çš„å•é¡Œ
    if previous_answers and question_number > 0:
        context = ""
        for i, answer in enumerate(previous_answers):
            context += f"å•é¡Œ{i+1}å›ç­”ï¼š{answer[:50]}{'...' if len(answer) > 50 else ''}\n"
        
        if question_number < len(base_prompts):
            prompt = f"""
åŸºæ–¼ä½¿ç”¨è€…ä¹‹å‰çš„å›ç­”ï¼š
{context}

{base_prompts[question_number]}
è«‹ç¢ºä¿ï¼š
1. å•é¡Œé•·åº¦æ§åˆ¶åœ¨15-25å­—
2. é¿å…èˆ‡ä¹‹å‰å•é¡Œé‡è¤‡
3. å•é¡Œè¦å…·é«”ä¸”æ˜“æ–¼å›ç­”
4. åªå›å‚³å•é¡Œæœ¬èº«ï¼Œä¸è¦å…¶ä»–èªªæ˜
"""
        else:
            # è¶…éåŸºç¤å•é¡Œæ™‚ï¼Œæ ¹æ“šå‰é¢å›ç­”ç”Ÿæˆæ·±åº¦å¿ƒç†å•é¡Œ
            prompt = f"""
åŸºæ–¼ä½¿ç”¨è€…ä¹‹å‰çš„å›ç­”ï¼š
{context}

è«‹ç”Ÿæˆä¸€å€‹æ·±å…¥çš„æŠ•è³‡å¿ƒç†å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œå°ˆæ³¨äº†è§£ä½¿ç”¨è€…çš„ï¼š
- æŠ•è³‡æ™‚çš„å…§å¿ƒæ„Ÿå—å’Œæƒ…ç·’
- é¢å°è™§ææ™‚çš„å¿ƒç†åæ‡‰
- æŠ•è³‡å£“åŠ›çš„ä¾†æºå’Œå½±éŸ¿
- æƒ…ç·’å¦‚ä½•å½±éŸ¿æŠ•è³‡æ±ºç­–
- æŠ•è³‡ç„¦æ…®å’Œææ‡¼çš„å…·é«”è¡¨ç¾

è¦æ±‚ï¼š
1. å•é¡Œé•·åº¦15-25å­—
2. èšç„¦å¿ƒç†å’Œæƒ…ç·’å±¤é¢ï¼Œä¸è¦å•é‡‘é¡æˆ–ç­–ç•¥
3. é¿å…é‡è¤‡ä¹‹å‰çš„å…§å®¹
4. åªå›å‚³å•é¡Œæœ¬èº«
"""
    else:
        # ç¬¬ä¸€å€‹å•é¡Œæˆ–æ²’æœ‰ä¸Šä¸‹æ–‡æ™‚ä½¿ç”¨åŸºç¤æç¤º
        if question_number < len(base_prompts):
            prompt = base_prompts[question_number] + "\nè¦æ±‚ï¼šåªå›å‚³å•é¡Œæœ¬èº«ï¼Œé•·åº¦15-25å­—ã€‚"
        else:
            prompt = "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡å¿ƒç†å£“åŠ›æˆ–æƒ…ç·’çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œäº†è§£ä½¿ç”¨è€…çš„æŠ•è³‡å¿ƒç†ç‹€æ…‹ã€‚åªå›å‚³å•é¡Œæœ¬èº«ã€‚"
    
    return call_gemini(prompt, is_question=True)

# === åˆ†ææ¨¡å‹ç›¸é—œå‡½å¼ ===
def sanitize_sentiment_output(raw) -> Dict[str, float]:
    """è§£æ SentimentModel è¼¸å‡ºï¼Œæå– negativeã€neutralã€positive åˆ†æ•¸"""
    result = {"negative": 0.0, "neutral": 0.0, "positive": 0.0}
    
    try:
        # è™•ç†å·¢ç‹€åˆ—è¡¨æ ¼å¼: [[{...}]]
        if isinstance(raw, list) and raw and isinstance(raw[0], list):
            data = raw[0]
        elif isinstance(raw, list):
            data = raw
        else:
            return result
            
        for item in data:
            if not isinstance(item, dict):
                continue
                
            label = item.get("label", "").lower()
            score = float(item.get("score", 0.0))
            
            if "negative" in label or "neg" in label:
                result["negative"] = score
            elif "positive" in label or "pos" in label:
                result["positive"] = score
            elif "neutral" in label or "neu" in label:
                result["neutral"] = score
                
    except Exception as e:
        print(f"è§£ææƒ…ç·’è¼¸å‡ºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
    return result

def sanitize_stress_output(raw) -> Dict[str, float]:
    """è§£æ StressModel è¼¸å‡ºï¼Œæå– stressã€not_stress åˆ†æ•¸"""
    result = {"stress": 0.0, "not_stress": 0.0}
    
    try:
        # è™•ç†å·¢ç‹€åˆ—è¡¨æ ¼å¼: [[{...}]]
        if isinstance(raw, list) and raw and isinstance(raw[0], list):
            data = raw[0]
        elif isinstance(raw, list):
            data = raw
        else:
            return result
            
        for item in data:
            if not isinstance(item, dict):
                continue
                
            label = item.get("label", "").lower()
            score = float(item.get("score", 0.0))
            
            if "stressed" in label and "not" not in label:
                result["stress"] = score
            elif "not stressed" in label or "not" in label:
                result["not_stress"] = score
                
    except Exception as e:
        print(f"è§£æå£“åŠ›è¼¸å‡ºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
    return result

def analyze_user_response(text: str, question: str = "") -> tuple[Dict[str, float], Dict[str, float]]:
    """åˆ†æä½¿ç”¨è€…å›æ‡‰ï¼Œå›å‚³æƒ…ç·’å’Œå£“åŠ›åˆ†æ•¸"""
    global SENTIMENT_MODEL, STRESS_MODEL
    
    # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
    if SENTIMENT_MODEL is None:
        SENTIMENT_MODEL = SentimentModel()
    if STRESS_MODEL is None:
        STRESS_MODEL = StressModel()
    
    # çµ„åˆå•é¡Œå’Œå›ç­”æä¾›å®Œæ•´ä¸Šä¸‹æ–‡
    if question.strip():
        # æ ¼å¼: "å•é¡Œï¼š{question} å›ç­”ï¼š{text}" 
        analysis_text = f"å•é¡Œï¼š{question.strip()} å›ç­”ï¼š{text.strip()}"
    else:
        # å¦‚æœæ²’æœ‰å•é¡Œï¼Œå°±ç›´æ¥ç”¨å›ç­”
        analysis_text = text.strip()
    
    # åŸ·è¡Œåˆ†æï¼ˆä½¿ç”¨åŒ…å«ä¸Šä¸‹æ–‡çš„æ–‡æœ¬ï¼‰
    sentiment_raw = SENTIMENT_MODEL.analyze(analysis_text)
    stress_raw = STRESS_MODEL.analyze(analysis_text)
    
    # è§£æçµæœ
    sentiment_scores = sanitize_sentiment_output(sentiment_raw)
    stress_scores = sanitize_stress_output(stress_raw)
    
    return sentiment_scores, stress_scores

def generate_final_advice(all_sentiment_scores: List[Dict[str, float]], 
                         all_stress_scores: List[Dict[str, float]]) -> str:
    """æ ¹æ“šæ‰€æœ‰å•é¡Œçš„æƒ…ç·’èˆ‡å£“åŠ›åˆ†æ•¸ç”Ÿæˆæœ€çµ‚å»ºè­°"""
    
    # è¨ˆç®—å¹³å‡åˆ†æ•¸
    avg_sentiment = {"negative": 0.0, "neutral": 0.0, "positive": 0.0}
    avg_stress = {"stress": 0.0, "not_stress": 0.0}
    
    if all_sentiment_scores:
        for key in avg_sentiment:
            avg_sentiment[key] = sum(s.get(key, 0) for s in all_sentiment_scores) / len(all_sentiment_scores)
    
    if all_stress_scores:
        for key in avg_stress:
            avg_stress[key] = sum(s.get(key, 0) for s in all_stress_scores) / len(all_stress_scores)
    
    # å»ºç«‹åˆ†ææ‘˜è¦
    summary_lines = []
    for i, (sentiment, stress) in enumerate(zip(all_sentiment_scores, all_stress_scores), 1):
        summary_lines.append(
            f"å•é¡Œ{i}: è² é¢={sentiment.get('negative', 0):.3f}, "
            f"ä¸­æ€§={sentiment.get('neutral', 0):.3f}, "
            f"æ­£é¢={sentiment.get('positive', 0):.3f}, "
            f"å£“åŠ›={stress.get('stress', 0):.3f}, "
            f"ç„¡å£“åŠ›={stress.get('not_stress', 0):.3f}"
        )
    
    # ç”Ÿæˆ prompt çµ¦ Gemini
    prompt = f"""
è«‹æ ¹æ“šä»¥ä¸‹ä½¿ç”¨è€…åœ¨æŠ•è³‡ç†è²¡å•å·ä¸­çš„æƒ…ç·’èˆ‡å£“åŠ›åˆ†æçµæœï¼Œæä¾›å€‹äººåŒ–çš„ç†è²¡å»ºè­°èˆ‡å¿ƒç†å¥åº·å»ºè­°ï¼š

è©³ç´°åˆ†æçµæœï¼š
{chr(10).join(summary_lines)}

å¹³å‡æƒ…ç·’åˆ†æ•¸ï¼š
- è² é¢æƒ…ç·’: {avg_sentiment['negative']:.3f}
- ä¸­æ€§æƒ…ç·’: {avg_sentiment['neutral']:.3f}  
- æ­£é¢æƒ…ç·’: {avg_sentiment['positive']:.3f}

å¹³å‡å£“åŠ›æŒ‡æ¨™ï¼š
- æœ‰å£“åŠ›: {avg_stress['stress']:.3f}
- ç„¡å£“åŠ›: {avg_stress['not_stress']:.3f}

è«‹æä¾›ï¼š
1. æŠ•è³‡å¿ƒç†ç‹€æ…‹åˆ†æ
2. å€‹äººåŒ–ç†è²¡å»ºè­°ï¼ˆåŒ…å«é¢¨éšªç®¡ç†ã€è³‡ç”¢é…ç½®ç­‰ï¼‰
3. å£“åŠ›ç®¡ç†èˆ‡æƒ…ç·’èª¿é©å»ºè­°
4. å…·é«”çš„è¡Œå‹•æ–¹æ¡ˆ

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œå…§å®¹è¦å¯¦ç”¨ä¸”æ˜“æ–¼åŸ·è¡Œã€‚
æ³¨æ„ï¼šè«‹ä¸è¦ä½¿ç”¨ä»»ä½• Markdown æ ¼å¼æ¨™è¨˜ï¼ˆå¦‚ ** ç²—é«”æ¨™è¨˜ï¼‰ï¼Œå›ç­”å…§å®¹æ‡‰è©²æ˜¯ç´”æ–‡å­—æ ¼å¼ã€‚
    """
    
    advice_text = call_gemini(prompt, is_question=False)
    
    # ç§»é™¤æ‰€æœ‰çš„ Markdown æ ¼å¼æ¨™è¨˜
    clean_advice = advice_text.replace("**", "").replace("*", "")
    
    return clean_advice

# === API ç«¯é» ===
@app.get("/models")
def get_models_info():
    """å–å¾—æ¨¡å‹è³‡è¨Šï¼ˆé™¤éŒ¯ç”¨ï¼‰"""
    return {
        "model_name": MODEL_NAME,
        "api_key_set": bool(GOOGLE_API_KEY),
        "total_questions": TOTAL_QUESTIONS
    }

@app.post("/start", response_model=StartResponse)
def start_test():
    """é–‹å§‹æ–°æ¸¬é©—"""
    session_id = str(uuid.uuid4())
    
    # å»ºç«‹ sessionï¼ˆä¸ç”Ÿæˆç¬¬ä¸€å€‹å•é¡Œï¼Œæ”¹ç”±ä¸²æµç«¯é»è™•ç†ï¼‰
    with SESSIONS_LOCK:
        SESSIONS[session_id] = {
            "question_index": 0,
            "questions": [],
            "answers": [],
            "sentiment_scores": [],
            "stress_scores": [],
        }
    
    return StartResponse(session_id=session_id)

@app.post("/answer", response_model=NextQuestionResponse)
def submit_answer(req: AnswerRequest):
    """æäº¤å›ç­”"""
    # é©—è­‰ session
    with SESSIONS_LOCK:
        if req.session_id not in SESSIONS:
            raise HTTPException(status_code=404, detail="Session ä¸å­˜åœ¨")
        session = SESSIONS[req.session_id]
        
        # å–å¾—ç•¶å‰å•é¡Œï¼ˆç”¨æ–¼æä¾›ä¸Šä¸‹æ–‡çµ¦æ¨¡å‹åˆ†æï¼‰
        current_question = ""
        if session["question_index"] < len(session["questions"]):
            current_question = session["questions"][session["question_index"]]
    
    # åˆ†æä½¿ç”¨è€…å›æ‡‰ï¼ˆåŒ…å«å•é¡Œä¸Šä¸‹æ–‡ï¼‰
    sentiment_scores, stress_scores = analyze_user_response(req.answer_text, current_question)
    
    # æ›´æ–° session
    with SESSIONS_LOCK:
        session["answers"].append(req.answer_text)
        session["sentiment_scores"].append(sentiment_scores)
        session["stress_scores"].append(stress_scores)
        session["question_index"] += 1
        
        current_question_index = session["question_index"]
    
    # æª¢æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰å•é¡Œ
    if current_question_index >= TOTAL_QUESTIONS:
        # ç”Ÿæˆæœ€çµ‚å»ºè­°
        advice = generate_final_advice(session["sentiment_scores"], session["stress_scores"])
        
        # æ¸…ç† session
        with SESSIONS_LOCK:
            del SESSIONS[req.session_id]
        
        return NextQuestionResponse(
            next_question=None,
            finished=True,
            advice=advice
        )
    else:
        # ç”Ÿæˆä¸‹ä¸€å€‹å•é¡Œï¼ˆå¸¶æœ‰ä¹‹å‰å›ç­”çš„ä¸Šä¸‹æ–‡ï¼‰
        next_question = generate_question(current_question_index, session["answers"])
        
        # æ›´æ–° session
        with SESSIONS_LOCK:
            session["questions"].append(next_question)
        
        return NextQuestionResponse(
            next_question=next_question,
            finished=False,
            advice=None
        )

@app.post("/stream-question")
async def stream_question(req: StreamQuestionRequest):
    """ä¸²æµç”Ÿæˆå•é¡Œ"""
    # é©—è­‰ session
    with SESSIONS_LOCK:
        if req.session_id not in SESSIONS:
            raise HTTPException(status_code=404, detail="Session ä¸å­˜åœ¨")
        session = SESSIONS[req.session_id]
        previous_answers = session.get("answers", [])
    
    # æ ¹æ“šå•é¡Œç·¨è™Ÿå’Œä¹‹å‰çš„å›ç­”ç”Ÿæˆå•é¡Œ
    question_number = req.question_number
    
    # å»ºç«‹ prompt
    base_prompts = [
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡æ™‚å¿ƒç†å£“åŠ›çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…æŠ•è³‡æ™‚çš„å£“åŠ›æ„Ÿå—ã€‚",
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡æƒ…ç·’æ³¢å‹•çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…é¢å°å¸‚å ´è®ŠåŒ–æ™‚çš„æƒ…ç·’åæ‡‰ã€‚",
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡ç„¦æ…®çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…åœ¨æŠ•è³‡éç¨‹ä¸­çš„æ“”æ†‚å’Œææ‡¼ã€‚",
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡æ±ºç­–å¿ƒç†çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…åšæŠ•è³‡æ±ºå®šæ™‚çš„å¿ƒç†ç‹€æ…‹ã€‚",
        "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡å£“åŠ›ç®¡ç†çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œç›®çš„:äº†è§£ä½¿ç”¨è€…å¦‚ä½•è™•ç†æŠ•è³‡å¸¶ä¾†çš„å¿ƒç†è² æ“”ã€‚"
    ]
    
    if previous_answers and question_number > 0:
        context = ""
        for i, answer in enumerate(previous_answers):
            context += f"å•é¡Œ{i+1}å›ç­”ï¼š{answer[:50]}{'...' if len(answer) > 50 else ''}\n"
        
        if question_number < len(base_prompts):
            prompt = f"""
åŸºæ–¼ä½¿ç”¨è€…ä¹‹å‰çš„å›ç­”ï¼š
{context}

{base_prompts[question_number]}
è«‹ç¢ºä¿ï¼š
1. å•é¡Œé•·åº¦æ§åˆ¶åœ¨15-25å­—
2. é¿å…èˆ‡ä¹‹å‰å•é¡Œé‡è¤‡
3. å•é¡Œè¦å…·é«”ä¸”æ˜“æ–¼å›ç­”
4. åªå›å‚³å•é¡Œæœ¬èº«ï¼Œä¸è¦å…¶ä»–èªªæ˜
"""
        else:
            prompt = f"""
åŸºæ–¼ä½¿ç”¨è€…ä¹‹å‰çš„å›ç­”ï¼š
{context}

è«‹ç”Ÿæˆä¸€å€‹æ·±å…¥çš„æŠ•è³‡å¿ƒç†å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œå°ˆæ³¨äº†è§£ä½¿ç”¨è€…çš„ï¼š
- æŠ•è³‡æ™‚çš„å…§å¿ƒæ„Ÿå—å’Œæƒ…ç·’
- é¢å°è™§ææ™‚çš„å¿ƒç†åæ‡‰
- æŠ•è³‡å£“åŠ›çš„ä¾†æºå’Œå½±éŸ¿
- æƒ…ç·’å¦‚ä½•å½±éŸ¿æŠ•è³‡æ±ºç­–
- æŠ•è³‡ç„¦æ…®å’Œææ‡¼çš„å…·é«”è¡¨ç¾

è¦æ±‚ï¼š
1. å•é¡Œé•·åº¦15-25å­—
2. èšç„¦å¿ƒç†å’Œæƒ…ç·’å±¤é¢ï¼Œä¸è¦å•é‡‘é¡æˆ–ç­–ç•¥
3. é¿å…é‡è¤‡ä¹‹å‰çš„å…§å®¹
4. åªå›å‚³å•é¡Œæœ¬èº«
"""
    else:
        if question_number < len(base_prompts):
            prompt = base_prompts[question_number] + "\nè¦æ±‚ï¼šåªå›å‚³å•é¡Œæœ¬èº«ï¼Œé•·åº¦15-25å­—ã€‚"
        else:
            prompt = "è«‹ç”Ÿæˆä¸€å€‹é—œæ–¼æŠ•è³‡å¿ƒç†å£“åŠ›æˆ–æƒ…ç·’çš„ç°¡çŸ­å•é¡Œï¼ˆ15-25å­—ï¼‰ï¼Œäº†è§£ä½¿ç”¨è€…çš„æŠ•è³‡å¿ƒç†ç‹€æ…‹ã€‚åªå›å‚³å•é¡Œæœ¬èº«ã€‚"
    
    # å›å‚³ä¸²æµå•é¡Œ
    return StreamingResponse(
        stream_question_generation(prompt),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

@app.post("/save-question")
def save_question(req: dict):
    """ä¿å­˜ä¸²æµç”Ÿæˆçš„å•é¡Œåˆ° session"""
    session_id = req.get("session_id")
    question = req.get("question")
    
    if not session_id or not question:
        raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦åƒæ•¸")
    
    with SESSIONS_LOCK:
        if session_id in SESSIONS:
            SESSIONS[session_id]["questions"].append(question)
            return {"message": "å•é¡Œå·²ä¿å­˜"}
        else:
            raise HTTPException(status_code=404, detail="Session ä¸å­˜åœ¨")

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚åŸ·è¡Œ"""
    global SENTIMENT_MODEL, STRESS_MODEL
    
    print("æ­£åœ¨è¼‰å…¥åˆ†ææ¨¡å‹...")
    
    try:
        if SENTIMENT_MODEL is None:
            SENTIMENT_MODEL = SentimentModel()
            print("âœ… SentimentModel è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  SentimentModel è¼‰å…¥å¤±æ•—: {e}")
    
    try:
        if STRESS_MODEL is None:
            STRESS_MODEL = StressModel()
            print("âœ… StressModel è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  StressModel è¼‰å…¥å¤±æ•—: {e}")
    
    print(f"ğŸš€ ç†è²¡å•å· API å•Ÿå‹•å®Œæˆ (æ¨¡å‹: {MODEL_NAME})")

@app.get("/")
def root():
    """æ ¹è·¯å¾‘"""
    return {"message": "ç†è²¡å•å· API æœå‹™", "version": "1.0.0", "endpoints": ["/start", "/answer", "/models"]}

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)